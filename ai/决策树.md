
# 决策树

决策树就是一种无参数机器学习算法

熵

- P越小，则该事件发生带来的信息越大，信息量越大；
- 熵表示不确定性的度量，变量越随机，熵越大,信息量越大；

$$H(P)=-\sum_xP(x)logP(x)$$

![batch norm](img/Binary_entropy_plot.png)


条件熵 在一定条件下，随机变量的不确定性

$$ H(Y|X)= -\sum_{x,y}P(x)P(y|x)logP(y|x) $$


ID3 信息增益 = 熵-条件熵


$$Ent(D)=-\sum_{k=1}^{|y|}p_klogp_k$$
$$Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{D^v}{D}Ent(D^v)$$

C4.5 增益率

$$Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$
$$IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log\frac{|D^v|}{|D|}$$

增益率对可取值数目较少的属性有所偏好；先从候选属性中找出信息增益高于平均水平的属性，再选择增益率高的

## CART 

- CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。
- 回归树用平方误差最小化准则，分类树用基尼指数最小化准则
- 基尼指数衡量数据D的纯度，即类别标记不一致的概率

$$ Gini(p)=1-\sum_kp_k^2$$


## GDBT

- 梯度下降树 
- 分类树的结果显然是没办法累加的，所以GBDT中的树都是回归树，不是分类树
- 集成学习Boosting家族的成员 
- 弱学习器限定了只能使用CART回归树模型
- 前一轮x学习器为$f_{t-1}(x)$,损失为$L(y, f_{t-1})$，本轮迭代目标是让$L(y, f_{t})=L(y, f_{t-1}+f_t)$最小
- 用损失函数的负梯度来拟合本轮损失的近似值

## GBDT与随机森林

- 都是由多棵树组成，最终的结果都是由多棵树一起决定
- 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成
- 组成随机森林的树可以并行生成；而GBDT只能是串行生成
- 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来
- 随机森林对异常值不敏感，GBDT对异常值非常敏感
- 随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成
- 随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能