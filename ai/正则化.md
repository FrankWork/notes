# 防止过拟合

提前终止，L1 L2正则化，soft weight sharing， 增加数据
dropout, batch norm

# dropout

no drop:

$$ z_{i}^{l}=w_{i}^{l}y^{l-1} $$
$$ y_{i}^{l}=f(z_{i}^{l})$$

drop:

$$ r_{j}^{l} \sim Bernoulli(p) $$
$$ z_{i}^{l}=w_{i}^{l}y^{l-1}*r^{l-1} $$
$$ y_{i}^{l}=f(z_{i}^{l})$$

测试时$w_{i}^{l}=p*w_{i}^{l}$, $p$为keep prob

## dropout 为什么有用

- 减少神经元之间复杂的相关关系
- 噪声，相当于数据增强
- 产生的向量具有稀疏性
- 看作是模型平均的一种。对于每次输入到网络中的样本，其对应的网络结构都是不同的
- 朴素贝叶斯假设各个特征之间相互独立，单独对每个特征进行学习，测试时将所有的特征都相乘
  dropout每次训练一部分隐含层特征

## dropout 测试时 rescale 

当模型使用了dropout layer，训练的时候只有占比为$p$的隐藏层单元参与训练。
那么在预测的时候，如果所有的隐藏层单元都需要参与进来，则得到的结果相比训练时平均要大$\frac{1}{p}$，
为了避免这种情况，就需要测试的时候将输出结果乘以$p$使下一层的输入规模保持不变。


## inverted dropout

利用inverted dropout，我们可以在训练的时候直接将dropout后留下的权重扩大$\frac{1}{p}$倍，这样就可以使结果的scale保持不变，而在预测的时候也不用做额外的操作了，更方便一些。

比例因子将修改为是保留概率的倒数

$$ r_{j}^{l} \sim Bernoulli(p) $$
$$ z_{i}^{l}=w_{i}^{l}y^{l-1}*r^{l-1} $$
$$ y_{i}^{l}=\frac{1}{p}f(z_{i}^{l})$$


## rnn dorpout

1. 将dropout用于非循环的连接，即上下层连接
2. 在每个时间点重复同样的dropout mask，包括输入、输出和循环层（每个时间点舍去相同的网络单元）


# batch norm

