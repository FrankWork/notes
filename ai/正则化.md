# 防止过拟合

提前终止，L1 L2正则化，soft weight sharing， 增加数据
dropout, batch norm

# dropout

no drop:

$$ z_{i}^{l}=w_{i}^{l}y^{l-1} $$
$$ y_{i}^{l}=f(z_{i}^{l})$$

drop:

$$ r_{j}^{l} \sim Bernoulli(p) $$
$$ z_{i}^{l}=w_{i}^{l}y^{l-1}*r^{l-1} $$
$$ y_{i}^{l}=f(z_{i}^{l})$$

测试时$w_{i}^{l}=p*w_{i}^{l}$, $p$为keep prob

## dropout 为什么有用

- 减少神经元之间复杂的相关关系
- 噪声，相当于数据增强
- 产生的向量具有稀疏性
- 看作是模型平均的一种。对于每次输入到网络中的样本，其对应的网络结构都是不同的
- 朴素贝叶斯假设各个特征之间相互独立，单独对每个特征进行学习，测试时将所有的特征都相乘
  dropout每次训练一部分隐含层特征

## dropout 测试时 rescale 

当模型使用了dropout layer，训练的时候只有占比为$p$的隐藏层单元参与训练。
那么在预测的时候，如果所有的隐藏层单元都需要参与进来，则得到的结果相比训练时平均要大$\frac{1}{p}$，
为了避免这种情况，就需要测试的时候将输出结果乘以$p$使下一层的输入规模保持不变。


## inverted dropout

利用inverted dropout，我们可以在训练的时候直接将dropout后留下的权重扩大$\frac{1}{p}$倍，这样就可以使结果的scale保持不变，而在预测的时候也不用做额外的操作了，更方便一些。

比例因子将修改为是保留概率的倒数

$$ r_{j}^{l} \sim Bernoulli(p) $$
$$ z_{i}^{l}=w_{i}^{l}y^{l-1}*r^{l-1} $$
$$ y_{i}^{l}=\frac{1}{p}f(z_{i}^{l})$$


## rnn dorpout

1. 将dropout用于非循环的连接，即上下层连接
2. 在每个时间点重复同样的dropout mask，包括输入、输出和循环层（每个时间点舍去相同的网络单元）


# batch norm

![batch norm](img/batch_norm.jpg)

1. 输出规范化：结果各个维度上均值为0，方差为1
2. 缩放平移：让bn能还原最初的输入，保证网络的学习能力


## cnn

在CNN中，BN应作用在非线性映射前，即对$x=Wu+b$做规范化。另外对CNN的“权值共享”策略

## bn为什么效果好

1. 保持深度网络各层的输入分布不变
2. 数据伸缩不变性。将激活层输出规范为均值和方差一致，防止结果落到激活函数的饱和区，防止梯度消失，简化对学习率的选择
3. 权重伸缩不变性。权重$W$伸缩时，对应的均值和方差等比例伸缩，通过bn规范化，不会导致梯度消失或爆炸。减少对权重初始化的依赖



# L1和L2

## L1和L2 regularization原理，

## 为什么L1可以使参数优化到0