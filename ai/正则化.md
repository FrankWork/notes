# 防止过拟合

提前终止，L1 L2正则化，soft weight sharing， 增加数据
dropout, batch norm

# dropout

no drop:

$$ z_{i}^{l}=w_{i}^{l}y^{l-1} $$
$$ y_{i}^{l}=f(z_{i}^{l})$$

drop:

$$ r_{j}^{l} \sim Bernoulli(p) $$
$$ z_{i}^{l}=w_{i}^{l}y^{l-1}*r^{l-1} $$
$$ y_{i}^{l}=f(z_{i}^{l})$$

测试时$w_{i}^{l}=p*w_{i}^{l}$, $p$为keep prob

## dropout 为什么有用

- 减少神经元之间复杂的相关关系
- 噪声，相当于数据增强
- 产生的向量具有稀疏性
- 看作是模型平均的一种。对于每次输入到网络中的样本，其对应的网络结构都是不同的
- 朴素贝叶斯假设各个特征之间相互独立，单独对每个特征进行学习，测试时将所有的特征都相乘
  dropout每次训练一部分隐含层特征

## dropout 测试时 rescale 

当模型使用了dropout layer，训练的时候只有占比为$p$的隐藏层单元参与训练。
那么在预测的时候，如果所有的隐藏层单元都需要参与进来，则得到的结果相比训练时平均要大$\frac{1}{p}$，
为了避免这种情况，就需要测试的时候将输出结果乘以$p$使下一层的输入规模保持不变。


## inverted dropout

利用inverted dropout，我们可以在训练的时候直接将dropout后留下的权重扩大$\frac{1}{p}$倍，这样就可以使结果的scale保持不变，而在预测的时候也不用做额外的操作了，更方便一些。

比例因子将修改为是保留概率的倒数

$$ r_{j}^{l} \sim Bernoulli(p) $$
$$ z_{i}^{l}=w_{i}^{l}y^{l-1}*r^{l-1} $$
$$ y_{i}^{l}=\frac{1}{p}f(z_{i}^{l})$$


## rnn dorpout

1. 将dropout用于非循环的连接，即上下层连接
2. 在每个时间点重复同样的dropout mask，包括输入、输出和循环层（每个时间点舍去相同的网络单元）


# batch norm

![batch norm](img/batch_norm.jpg)

1. 输出规范化：结果各个维度上均值为0，方差为1
2. 缩放平移：让bn能还原最初的输入，保证网络的学习能力


## cnn

在CNN中，BN应作用在非线性映射前，即对$x=Wu+b$做规范化。另外对CNN的“权值共享”策略

## bn为什么效果好

1. 保持深度网络各层的输入分布不变
2. 数据伸缩不变性。将激活层输出规范为均值和方差一致，防止结果落到激活函数的饱和区，防止梯度消失，简化对学习率的选择
3. 权重伸缩不变性。权重$W$伸缩时，对应的均值和方差等比例伸缩，通过bn规范化，不会导致梯度消失或爆炸。减少对权重初始化的依赖



# L1和L2

## L1和L2 regularization原理，

1. L1范数是指向量中各个元素绝对值之和，可以实现稀疏
  - L1可以来选择特征, 对于强相关特征只会保留一个
  - L1范数近似求解L0范数，实现稀疏化。L0范数表示向量中非零元素的个数，
    L0范数优化求解问题（L0范数最小化）属于NP难问题

2. L2 向量中各个元素的平方和，权值衰减
  - 不能直接特征选择，只能约束系数大小
  - L2解决过拟合，使得W的每个元素都很小，越小的参数说明模型越简单，越简单的模型越不容易过拟合
  - w越小，输入发生微小变化的时候，输出会发生的变化越小。也就是系统对微小变化的敏感度低

## 为什么L1可以使参数优化到0

l1对于小值的惩罚，比l2的大

$$\frac{\partial L1}{\partial w}=sgn(w)$$
$$\frac{\partial L2}{\partial w}=2*w$$

阶跃函数 sgn

$$sgn(x)=
\begin{cases}
-1 & \text{x<0} \\
0 & \text{x=0} \\
1 & \text{x>0}
\end{cases}$$


l1在0处不可导，可以用近端梯度下降，近似优化损失函数的二阶泰勒展开式，相当于优化损失函数+l1
