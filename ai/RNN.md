# rnn

## basic rnn


$$ h_{t}=tanh(Wh^{t-1}+Ux^{t}+b) $$
$$o^{t}=Vh^{t}+c$$
$$\hat{y}^t=\sigma(o^t)$$
$$L=\sum_{t=1}L^t$$

1. RNN为什么要对隐藏层的输出h进行一次映射，而不在h上直接softmax?
    - h的维度和类别数量不同
2. U,V,W三个参数的意义
    - U将输入映射到隐藏层空间，隐藏层相当于模型的记忆单元
    - W是隐藏层空间到自身的映射，定义了如何利用上下文,利用之前的记忆；U,W结合了输入和记忆形成了当前的状态
    - V隐藏层到输出空间的映射
3. BPTT和BP区别在哪
    - BP只考虑了层级之间的梯度的纵向传递
    - BPTT考虑了层级之间的梯度的纵向传递和时间上的横向传递

## lstm

遗忘门，输入门, 候选值, 遗忘与更新操作，输出门，输出操作

$$ f_{t} = \sigma(W_{f} \cdot[h_{t-1},x_{t}+b_{f}]) $$
$$ i_{t} = \sigma(W_{i} \cdot[h_{t-1},x_{t}+b_{i}]) $$
$$ \tilde{C_{t}} = tanh(W_{C} \cdot[h_{t-1},x_{t}+b_{C}]) $$
$$ C_{t} = f_{t}*C_{t-1} + i_{t}*\tilde{C_{t}}$$
$$ o_{t} = \sigma(W_{o} \cdot[h_{t-1},x_{t}+b_{o}]) $$
$$ h_{t}=o_{t}*tanh(C_{t}) $$



## 梯度裁剪(梯度爆炸)

1. 当梯度的绝对值大于某个上限的时候，就剪裁为上限
2. 梯度的L2范数大于上限后，让梯度除以范数


## lstm为什么比rnn好（梯度消失）

- LSTM只能避免RNN的梯度消失
- 传统的RNN以覆盖的方式计算状态$h_{t}=f(h_{t-1},x_{t})$，导致梯度为参数的连乘积的形式，小于1的项连乘就会趋近于0.
- LSTMS使用累加的形式计算状态$h_{t}=\sum_{i=1}^{t}\Delta h_{i}$，这种累加形式的导数也是累加形式,避免了梯度消失
