## 什么样的资料集不适合用深度学习?

1. 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。
2 .数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。

## 何为共线性, 跟过拟合有啥关联?

1. 多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。
2. 共线性会造成冗余，导致过拟合。
3. 解决方法：排除变量的相关性／加入权重正则

## 对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法?

对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好
也就是说：对于所有问题，无论学习算法A多聪明，学习算法B多笨拙，它们的期望性能相同。
但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。

## 梯度消失问题

1. 前馈神经网络（包括全连接层、卷积层等）可以表示为 

$$F = f_{3} (f_{2} (f_{1} (xW_{1})W_{2})W_{3})$$

网络输出对$W_{1}$求偏导

$$ \frac{\partial F}{\partial W_{1}}= x*f'_{1}*W_{2}*f'_{2}*W_{3}*f'_{3} $$

这里$W_{1}W_{2}W_{3}$ 是相互独立的，一般不会有数值问题；主要问题在于激活函数的导数$f'$在饱和区接近于零，导致梯度消失。

2. 循环神经网络的状态循环部分可以表示为

$$ h_{3}=f_{3} (f_{2} (f_{1} (h_{0}W)W)W) $$

这里的问题不仅在于激活函数的导数，还有$W$在不同时刻是共享的，网络输出对$W$的偏导包含$W$的连乘项，（$W$值偏小或偏大）就会出现梯度消失或爆炸

## 广义线性模型是怎被应用在深度学习中?

深度学习从统计学角度，可以看做递归的广义线性模型。
广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。
深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。
逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数


## 矩阵行列式的物理意义

行列式就是矩阵对应的线性变换对空间的拉伸程度的度量，或者说物体经过变换前后的体积比

## 深层网络可以避开较差Local Optima


## 量化句子之间的相似度

- word2vec 向量相加，bag of words
- TF-IDF，余弦相似度
- SVD和LSI
- LDA

## 生成模型　判别模型

1. 生成方法的特点：
- 生成方法学习联合概率密度分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。- 但它不关心到底划分各类的那个分类边界在哪。
- 生成方法可以还原出联合概率分布P(Y|X)，而判别方法不能。
- 生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，
- 当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。

2. 判别方法的特点：
- 判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y|X)。不能反映训练数据本身的特性。
- 但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。
- 直接面对预测，往往学习的准确率更高。
- 由于直接学习P(Y|X)或P(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。

3. 由生成模型可以得到判别模型，但由判别模型得不到生成模型

- 生成方法有混合高斯模型、朴素贝叶斯法和隐形马尔科夫模型等，
- 常见的判别方法有SVM、LR等

## 回归和分类

1. 回归　预测连续值　定量输出　分类算法得到是一个决策面

2. 分类 预测离散值　定性输出　回归算法得到是一个最优拟合线


## 性能度量

T, F 真假　P, N 正例　负例
- 查准率$ P = \frac{TP}{TP+FP} $
- 差全率$ R = \frac{TP}{TP+FN} $
- $ F1 = \frac{2PR}{P+R} $

TPR=TP/(TP+FN)
FPR=FP/(FP+TN)
AUC可以通过ROC曲线的个部分的面积之和得到

## 类别不平衡

假设正例不足
- 正例过采样　（增加正例，正例插值）
- 负例欠采样
- 在原始数据上训练，然后阈值漂移　（预测几率高于观测几率）

## 交叉验证

- 降低方差
- K=1,所以数据都被用于训练，模型很容易出现过拟合，因此容易是低偏差、高方差
- K=n。随着K值的不断升高，单一模型评估时的方差逐渐加大而偏差减小。但从总体模型角度来看，反而是偏差升高了而方差降低
- 所以当K值在1到n之间的游走，可以理解为一种方差和偏差妥协的结果

## 为什么要做数据归一化？

- 特征的量纲可能不同，导致某些指标被忽视
- 有些模型在各个维度进行不均匀伸缩后，最优解与原来不等价，例如SVM。对于这样的模型，除非本来各维数据的分布范围就比较接近，否则必须进行标准化，以免模型参数被分布范围较大或较小的数据dominate。
- 有些模型在各个维度进行不均匀伸缩后，最优解与原来等价，例如logistic regression。对于这样的模型，是否标准化理论上不会改变最优解。但是，由于实际求解往往使用迭代算法，如果目标函数的形状太“扁”，迭代算法可能收敛得很慢甚至不收敛。所以对于具有伸缩不变性的模型，最好也进行数据标准化。
