# word2vec

## 向量空间模型　VSM

- 文档的词频（而不是词序）代表了文档的主题（词袋假设）
- 上下文环境相似的两个词有着相近的语义（**分布假设**）

## Bengio神经网络语言模型　NNLM

1. 将one-hot词向量映射为稠密的词向量
2. 用前馈神经网络表示平滑的条件概率模型$p(w_{t}|context)$
3. 引入连续的词向量和平滑的概率模型，可以在一个连续空间里对序列概率进行建模，从而从根本上缓解数据稀疏性和维度灾难的问题

缺点：
* 只能处理定长序列　＝> (RNNLM)
* 训练太慢

## word2vec

将NNLM分为两步：
1. 用一个简单模型训练出连续的词向量
2. 基于词向量的表达，训练一个连续的Ngram神经网络

模型

1. - CBoW（Continuous Bag-of-Words Model）
   - 从context对target word的预测中学习到词向量的表达
2. - Skip-gram模型
   - 从target word对context的预测
   - 本质是计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化

层次Softmax

- 将复杂的归一化概率分解为一系列条件概率乘积
- 将原始大小为V的字典D转换成了一颗深度为logV的二叉树。树的叶子节点与原始字典里的word一一对应
- 基于Huffman编码的二叉树

负采样


