# word2vec

## one-hot编码缺点

1. 容易受维数灾难的困扰
2. 不能很好地刻画词与词之间的相似性

## 向量空间模型　VSM

- 文档的词频（而不是词序）代表了文档的主题（词袋假设）
- 上下文环境相似的两个词有着相近的语义（**分布假设**）
- 基于矩阵分解，将高纬的向量低纬化, 可以忽略绝对值接近0的值来达到降维的目的

## 分布表示　hinton

1. 每一个词映射成一个固定长度的短向量，将所有这些向量放在一起形成一个词向量空间
2. 在这个空间上引入“距离”，则可以根据词之间的距离来判断它们之间的（词法、语义上的）相似性了
3. 两个向量的夹角，就能用来计算向量之间的距离。这个就是我们说的余弦相似度


## Bengio神经网络语言模型　NNLM

1. 将one-hot词向量映射为稠密的词向量
2. 用前馈神经网络表示平滑的条件概率模型$p(w_{t}|context)$
3. 引入连续的词向量和平滑的概率模型，可以在一个连续空间里对序列概率进行建模，从而从根本上缓解数据稀疏性和维度灾难的问题

缺点：
* 只能处理定长序列　＝> (RNNLM)
* 训练太慢

## word2vec

将NNLM分为两步：
1. 用一个简单模型训练出连续的词向量
2. 基于词向量的表达，训练一个连续的Ngram神经网络

模型

1. - CBoW（Continuous Bag-of-Words Model）
   - 从context对target word的预测中学习到词向量的表达
2. - Skip-gram模型
   - 从target word对context的预测
   - 本质是计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化

层次Softmax

- 将复杂的归一化概率分解为一系列条件概率乘积
- 将原始大小为V的字典D转换成了一颗深度为logV的二叉树。树的叶子节点为原始字典里的word，有Ｖ个；非叶子节点有V-1个，对应着某一类word的集合
- 基于Huffman编码的二叉树

Huffman编码

出现概率高的符号使用较短的编码，出现概率低的符号则使用较长的编码，又称为最优二叉树，表示一种带权路径长度最短的二叉树

    假设有n个权值，则构造出来的Huffman树有n个叶子结点。若n个权值分别为{w1,w2,…,wn}。

    1. 将{w1,w2,…,wn}当做n棵树（每棵树1个结点）组成的森林。
    2. 选择根结点权值最小的两棵树，合并，获得一棵新树，且新树的根结点权值为其左、右子树根结点权值之和。词频大的结点作为左孩子结点，词频小的作为右孩子结点
    3. 从森林中删除被选中的树，保留新树。
    4. 重复2、3步，直至森林中只剩下一棵树为止

负采样

对高频词进行随机采样，高频词往往提供相对较少的信息，因此可以将高于特定词频的词语丢弃掉，以提高训练速度

采样概率: $t$为阈值，$f(w_{i})$为词频

$$ P(w_{i})=1-\sqrt{\frac{t}{f(w_{i})}} $$

# 词向量对比

