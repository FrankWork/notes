 初始化：   lecun_uniform /  glorot_normal / he_normal

## 权重初始化为0

前向传播时每一层的每一个神经元的输出都是一样的，后向传播时梯度也都是一样的，训练出来的权重也都相同

## 初始化为小的随机数 lecun

$$ Var(Y)=nVar(W_{i})Var(X_{i}) $$

- 权重初始化太大的化，会使输出的结果太大，落入激活函数的饱和区
- 权重太小，反向传播的梯度也会太小
- 校准方差，将权重初始化为小的随机数后，若输入的维度n过大，会使输出结果的方差过大，可以将权重除以$\sqrt{n}$，使权重的方差为$\frac{1}{n}$，输出的结果方差为1，避免输出落入激活函数的饱和区
- 偏置初始化为0


## Xavier/glorot

帮助信号能够在神经网络中传递得更深，让每一层的方差一致

- 前向传播时，想让$Var(W_{i})=\frac{1}{n_{in}}$
- 后向传播时，想让$Var(W_{i})=\frac{1}{n_{out}}$
- 折中：$Var(W_{i})=\frac{2}{n_{in}+n_{out}}$

权重分布：$W \sim U[-\frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}}]$

## kaiming he

Relu初始时一半梯度为0，让$Var(W_{i})=\frac{2}{n_{in}}$，放大一倍方差，来保持方差的平稳

