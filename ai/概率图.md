# 概率图
1. 有向图，贝叶斯网, 单向依赖
2. 无向图，马尔可夫网，互相依赖

# HMM　（有向图，生成式）

- 隐马尔可夫模型是结构最简单的动态贝叶斯网（有向图）
- 状态变量y, 观测变量x，
- 在任一时刻，观测变量的取值仅依赖与状态变量，$x_{t}$仅由$y_{t}$决定，ｘ之间彼此独立
- 在t时刻，状态变量$y_{t}$仅依赖于t-1时刻的$y_{t-1}$，即马尔可夫链(马尔科夫假设)
- 状态转移概率A，各个状态之间转移的概率
- 输出观测概率B，根据当前状态获得各个观测值的概率
- 初始状态概率$\pi$，模型在初始时刻各状态出现的概率

- 传统隐马尔科夫方法不易融合新特征

三个问题
1. - $\lambda=[A,B,\pi], x$ 求$p(x|\lambda)$
   - 概率计算问题， 前向后向算法
2. - $\lambda=[A,B,\pi], x$ 求$y$
   - 预测问题，维特比算法
3. - 已知$x$, 求$\lambda=[A,B,\pi]$，使$p(x|\lambda)$最大
   - 学习问题，极大似然估计，EM算法，即Baum-Welch算法

# 马尔可夫随机场　（无向图，生成式）

每个节点表示一个或一组变量，边表示变量之间的关系

# CRF (无向图，判别式)

- 目标是构建条件概率模型$P(y|x)$，y的分量之间可以有相关性
- 马尔可夫性：保证或者判断概率图是否为概率无向图的条件，a. 成对，b. 局部，c. 全局
- 能够使用复杂、有重叠性和非独立的特征进行训练和推理，能够充分地利用上下文信息作为特征，还可以任意地添加其他外部特征


1. HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关
2. CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布
3. HMM属于有向图，所以考虑了x与y的影响，但没讲x当做整体考虑进去。CRF属于无向图，没有这种依赖性，克服此问题。

## lstm crf

1. LSTM在序列建模上很强大，它们能够capture长远的上下文信息，此外还具备神经网络拟合非线性的能力，这些都是crf无法超越的地方, 一般的lstm输出层之间是相互独立的，不能对输出层之间的依赖进行建模
2. CRF：不能够考虑长远的上下文信息，它更多考虑的是整个句子的局部特征的线性加权组合（通过特征模版去扫描整个句子），但它优化的是整个序列（最终目标），而不是将每个时刻的最优拼接起来，在这一点上CRF要优于LSTM。
  适合小规模数据，比较难扩展，想在图模型上加边加圈，得重新推导公式和写代码

# 近似推断

1. 采样：马尔可夫链蒙特卡罗（MCMC）
2. 变分推断：用已知简单分布来逼近需推断的复杂分布

# LDA 

生成式有向图模型


https://www.zhihu.com/question/35866596


# EM算法


联合概率公式

$$P(X|Z)=\frac{P(X,Z)}{P(Z)}$$

全概率公式

$$P(X)=\sum_{i}P(X,Z_i) = \sum_{i}P(X|Z_i)P(Z_i)$$

理论上应该最大化完全数据的对数似然函数

$$LL(\Theta|X,Z)=lnP(X,Z|\Theta) $$

1. 但$Z$是隐变量，无法直接计算，转而求$Z$的期望，然后最大化不完全数据的对数似然函数

$$LL(\Theta|X)=lnP(X|\Theta)=\sum_ZP(X,Z|\Theta) $$

2. Q函数

$$Q(\Theta|\Theta^t)=\mathbb{E}_{Z|X,\Theta^t}LL(\Theta|X,Z) $$


## 高斯混合模型

- 完全数据的对数似然函数
- 确定Q函数
- 求偏导数为0，找到最大值点



