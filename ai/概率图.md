# 概率图
1. 有向图，贝叶斯网, 单向依赖
2. 无向图，马尔可夫网，互相依赖

# HMM　（有向图，生成式）

- 隐马尔可夫模型是结构最简单的动态贝叶斯网（有向图）
- 状态变量y, 观测变量x，
- 在任一时刻，观测变量的取值仅依赖与状态变量，$x_{t}$仅由$y_{t}$决定，ｘ之间彼此独立
- 在t时刻，状态变量$y_{t}$仅依赖于t-1时刻的$y_{t-1}$，即马尔可夫链(马尔科夫假设)
- 状态转移概率A，各个状态之间转移的概率
- 输出观测概率B，根据当前状态获得各个观测值的概率
- 初始状态概率$\pi$，模型在初始时刻各状态出现的概率

- 传统隐马尔科夫方法不易融合新特征

三个问题
1. 概率计算问题(前向后向算法), $\lambda=[A,B,\pi], x$ 求$p(x|\lambda)$
2. 预测问题(维特比算法)$\lambda=[A,B,\pi], x$ 求$y$
3. 学习问题
   - (EM算法，即Baum-Welch算法) 已知$x$, 求$\lambda=[A,B,\pi]$，使$p(x|\lambda)$最大
   - (极大似然估计，频数估计概率) 已知$x, y$

# 条件随机场 (无向图，判别式)

马尔可夫随机场，也叫概率无向图模型　（无向图，生成式）

- 由图表示的概率分布，每个节点表示一个或一组变量，边表示变量之间的关系
- 马尔可夫性：保证或者判断概率图是否为概率无向图的条件
  1. 成对: u,v之间没有边连接，给定$Y_O$时，则对应的随机变量$Y_u,Y_v$独立
  2. 局部：w为v有边连接的所有节点，给定$Y_w$时，则对应的随机变量$Y_O,Y_v$独立
  3. 全局: A,B是被点集合C分隔开的点集合，给定C时，则对应的随机变量$Y_A,Y_B$独立
- 因子分解：联合分布概率$P(Y)$表示为最大团上势函数的乘积形式

CRF

- 目标是构建条件概率模型$P(y|x)$，y的分量之间可以有相关性
- 每个位置局部特征求和，构造成全局特征
- 能够使用复杂、有重叠性和非独立的特征进行训练和推理，能够充分地利用上下文信息作为特征，还可以任意地添加其他外部特征
- 问题
  1. 概率计算问题: 前向后向算法， 前向后向向量
  2. 学习算法：最大似然估计（改进的迭代尺度法，牛顿法，拟牛顿法）
  3. 预测算法：维特比算法



1. HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关
2. CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布
3. HMM属于有向图，所以考虑了x与y的影响，但没讲x当做整体考虑进去。CRF属于无向图，没有这种依赖性，克服此问题。

## lstm crf

1. LSTM在序列建模上很强大，它们能够capture长远的上下文信息，此外还具备神经网络拟合非线性的能力，这些都是crf无法超越的地方, 一般的lstm输出层之间是相互独立的，不能对输出层之间的依赖进行建模
2. CRF：不能够考虑长远的上下文信息，它更多考虑的是整个句子的局部特征的线性加权组合（通过特征模版去扫描整个句子），但它优化的是整个序列（最终目标），而不是将每个时刻的最优拼接起来，在这一点上CRF要优于LSTM。
  适合小规模数据，比较难扩展，想在图模型上加边加圈，得重新推导公式和写代码

# 近似推断

1. 采样：马尔可夫链蒙特卡罗（MCMC）
2. 变分推断：用已知简单分布来逼近需推断的复杂分布

# LDA 

生成式有向图模型


https://www.zhihu.com/question/35866596


# EM算法


联合概率公式

$$P(X|Z)=\frac{P(X,Z)}{P(Z)}$$

全概率公式

$$P(X)=\sum_{i}P(X,Z_i) = \sum_{i}P(X|Z_i)P(Z_i)$$

理论上应该最大化完全数据的对数似然函数

$$LL(\Theta|X,Z)=lnP(X,Z|\Theta) $$

1. 但$Z$是隐变量，无法直接计算，转而求$Z$的期望，然后最大化不完全数据的对数似然函数

$$LL(\Theta|X)=lnP(X|\Theta)=\sum_ZP(X,Z|\Theta) $$

2. Q函数

$$Q(\Theta|\Theta^t)=\mathbb{E}_{Z|X,\Theta^t}LL(\Theta|X,Z) $$


## 高斯混合模型

- 完全数据的对数似然函数
- 确定Q函数
- 求偏导数为0，找到最大值点



