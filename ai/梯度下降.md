# 梯度下降

梯度下降算法是通过沿着目标函数$J(\theta)$参数$\theta$的梯度(一阶导数)相反方向，来不断更新模型参数来到达目标函数的极小值点

1. 全量梯度下降（Batch gradient descent）
2. 随机梯度下降(Stochastic gradient descent) 
3. 小批量梯度下降(Mini-batch gradient descent)


存在问题：
1. 选择一个合理的学习速率很难。学习速率过小，收敛速度很慢。学习速率过大，会阻碍收敛
2. 事先设定的学习速率调整，无法自适应每次学习的数据集特点
3. 对于非凸目标函数，容易陷入那些次优的局部极值点、鞍点中

batch大小的影响(mini-batch)：
- 增大batch，并行化效率提高了，内存不够；跑完一次数据集的迭代次数少了，但是参数更新次数少了，达到的精度不够；一定程度上，batch越大，下降方向越准


## Momentum

- 累积了前面的动量，获得更快的收敛速度与减少振荡
- 当前的梯度方向与上一次梯度方向相同，那么进行加强，在这些方向上更快了
- 当前的梯度方向与上一次梯度方向相反，那么进行削减，即这些方向上减慢了

$$\upsilon_{t}=\gamma\upsilon_{t-1}+\eta\nabla_{\theta}J(\theta)$$
$$\theta=\theta-\upsilon_{t},\qquad \gamma\le0.9$$


## NAG （Nesterov）涅斯捷罗夫梯度加速

- 不仅增加了动量项，还预估了下一次参数所在的位置
- 使用动量项加快收敛，还用预估的梯度修正当前下降的方向和大小

$$\upsilon_{t}=\gamma\upsilon_{t-1}+\eta\nabla_{\theta}J(\theta-\gamma\upsilon_{t-1})$$
$$\theta=\theta-\upsilon_{t},\qquad \gamma\le0.9$$


## Adagrad

- 对每个参数自适应不同的学习速率
- 对稀疏特征，得到大的学习更新
- 对非稀疏特征，得到较小的学习更新
- 因此该优化算法适合处理稀疏特征数据
- 缺点在于需要计算参数梯度序列平方和，并且学习速率趋势是不断衰减最终达到一个非常小的值

$$
\begin{aligned}
& \mathrm{g}_{t,i}=\nabla_{\theta}J(\theta_{i}) \\
& \theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{G_t+\epsilon}}\odot \mathrm{g}_t \quad \epsilon=1e-8\\
\end{aligned}
$$

$G_t$是一个对角矩阵,第i个对角元素为第i个参数从过去到当前的梯度平方和

## RMSprop

- 降低Adagrad中学习速率衰减过快问题
- 不计算梯度累积平方和，只利用前一时刻和当前时刻的梯度
- $\gamma=0.9$, $\eta=0.001$ 

$$
\begin{aligned}
& E[\mathrm{g}^2]_t=\gamma E[\mathrm{g}^2]_{t-1}+(1-\gamma)\mathrm{g}^2_t  \\
& \theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E[\mathrm{g}^2]_t+\epsilon}}\odot \mathrm{g}_t \quad \epsilon=1e-8\\
\end{aligned}
$$

## Adam (Adaptive Moment Estimation)

- 计算历史梯度衰减方式不同，不使用历史平方衰减，其衰减方式类似动量

梯度的带权平均和带权有偏方差

$$
\begin{aligned}
& m_t=\beta_1m_{t-1}+(1-\beta_1)\mathrm{g}_t  \\
& v_t=\beta_2v_{t-1}+(1-\beta_2)\mathrm{g}_t^2  \\
\end{aligned}
$$

$m_t$和$v_t$接近于0向量,对其进行修正

$$
\begin{aligned}
\hat{m}_t &=\frac{m_t}{1-\beta_1^t}\\
\hat{v}_t &=\frac{v_t}{1-\beta_2^t}\\
\theta_{t+1} &=\theta_t-\frac{\eta}{\sqrt{\hat{v}_t+\epsilon}}\hat{m}_t \\
\beta_1&=0.9, \beta_2=0.999, \epsilon=10^{-8} 
\end{aligned}
$$

