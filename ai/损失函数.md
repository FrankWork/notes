# softmax loss

交叉熵

$$ CrossEntropy=-\sum_{i}^{}P(i)logQ(i) $$

softmax

$$ a_{i}=\frac{e^{z_{i}}}{\sum_{j}{}e^{z_{j}}} $$

softmax loss

$$ L=-\sum_{i}^{C}y_{i}log(a_{i})=-log(a_{i})$$

梯度

$$ \frac{\partial L}{\partial w_{ki}}= \frac{\partial L}{\partial a_{i}} \frac{\partial a_{i}}{\partial z_{i}} \frac{\partial z_{i}}{\partial w_{ki}} = -\frac{1}{a_{i}}* a_{i}(1-a_{i})*o_{i} = (a_{i}-1)*o_{i} $$

$$ \frac{\partial L}{\partial w_{kj}}= -\frac{1}{a_{i}}* -a_{i}a_{j}*o_{i}=a_{i}*o_{i}$$


## 交叉熵和相对熵（KL散度）

KL散度是两个概率分布P和Q差别的非对称性的度量。典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。

$$ D_{KL}(P||Q)=\sum_{i}^{}P(i)logP(i)-\sum_{i}^{}P(i)logQ(i) $$

在机器学习中，由于真实的概率分布是固定的，前半部分是个常数。那么对KL散度的优化就相当于优化后半部分。
相对熵公式的后半部分就是交叉熵

$$ CrossEntropy=-\sum_{i}^{}P(i)logQ(i) $$

# Mean Squared Error(MSE)