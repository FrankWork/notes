
# LR

## LR为什么用sigmoid函数。这个函数有什么优点和缺点？为什么不用其他函数？

- 假设第i个特征对第k类的贡献是$w_{ki}$, 则点$(x_1,..,x_n)$属于k个概率正比于$exp(w_{k1}x_1,.., w_{kn}x_n)$
- 在二分类问题上，对这个概率正则化，就是sigmoid函数，参数w_i表示第i个特征对1类的贡献比对0类的贡献多多少

$$ P(y==1)=\frac{1}{1+e^{-\sum w_ix_i}}$$

## LR和SVM有什么区别

- 不考虑核函数，LR和SVM都是线性分类算法
- 都是判别模型
- 逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值
- 支持向量机​基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面
- 支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）
- 在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的, 假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的

## Logistics与随机森林比较

 logistic回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数g(z)将最为假设函数来预测



# svm

## SVM为什么要引入拉格朗日的优化方法。

SVM使用拉格朗日乘子法更为高效地求解了优化问题。

SVM将寻找具有最大几何间隔划分超平面的任务转化成一个凸优化问题,我们当然可以直接使用现成工具求解，但还有更为高效的方法，那就是使用拉格朗日乘子法将原问题转化为对偶问题求解。

具体做法是：（1）将约束融入目标函数中，得到拉格朗日函数；（2）然后对模型参数w和b求偏导，并令之为零；（3）得到w后，将其带入拉格朗日函数中，消去模型参数w和b；（4）这样就得到了原问题的对偶问题，对偶问题和原问题等价，同时对偶问题也是一个凸优化问题，使用SMO算法求解拉格朗日乘子；（5）得到拉格朗日乘子后，进一步可以得到模型参数w和b，也就得到了我们想要的划分超平面。

## smo算法

固定$a_i$之外的所有参数，求$a_i$上的极值
- 选取需更新的$a_i$和$a_j$
- 

## SVM在哪个地方引入的核函数,

如果原始空间线性不可分，可以将原始空间映射到高维空间，映射函数为f，在高维空间求解svm要算高维空间的內积，计算量比较大。若高维空间的內积可以表示为原始空间的通过某个函数计算出来的结果，这个函数叫做核函数。

线性核，多项式核，高斯核，sigmoid核，拉普拉斯核

## 如果用高斯核可以升到多少维。

高斯核 $exp(-||x-y||^2)$的泰勒展开是无穷的，可以映射到无穷维

## 软间隔
允许在一些样本上出错,但不满足约束的点应该尽量少

## SVM怎么防止过拟合

过拟合的办法是为SVM引入了松弛变量, 能够容忍异常点的存在

## 支持向量

距离超平面最近的几个训练样本点,两个异类支持向量到超平面的距离为间隔

## 目标函数

$$
\begin{aligned}
r&=\frac{|w^Tx+b|}{||w||}\\
&min\frac{1}{2}||w||^2

\end{aligned}
$$



# 随机森林

随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。

## 算法描述+bias和variance的分解公式


# HMM和CRF的本质区别

# 频率学派和贝叶斯派的本质区别

- * 频率学派认为某一事件的概率是常量，
  * 贝叶斯学派认为事件概率为变量
- * 频率学派将观测事件域中，某类事件发生的次数占总观测事件的比值近似为该事件发生的概率。
  * 贝叶斯学派则是在某些情况，如后验概率，条件概率的计算中加入超参
- * 频率学派以频率近似概率为核心思想，适合简单事件的概率分析，对于复杂的实际问题就不适用
  * 贝叶斯学派将概率本身视为变量，将该变量的超参独立出来, 可以解决频率学派无能为力的推理问题

# k近邻（knn）
找出训练集中与其最靠近的ｋ个训练样本，基于这ｋ个邻居来进行预测

回归：选取K个最近的点，然后平均（也可以加权平均）这些点y值

# k-means

K-means算法是将样本聚类成k个簇

## 与EM的关系

- 我们目的是将样本分成k个类，其实说白了就是求每个样例x的隐含类别y，然后利用隐含类别将x归类
- EM的思想，E步就是估计隐含类别y的期望值，M步调整其他参数使得在给定类别y的情况下，极大似然估计P(x,y)能够达到极大值
- 从K-means里我们可以看出它其实就是EM的体现，E步是确定隐含类别变量-最佳类别，M步更新其他参数使距离最小

# 决策树

ID3 信息增益 

$$Ent(D)=-\sum_{k=1}^{|y|}p_klogp_k$$
$$Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{D^v}{D}Ent(D^v)$$

C4.5 增益率

$$Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$
$$IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log\frac{|D^v|}{|D|}$$

增益率对可取值数目较少的属性有所偏好；先从候选属性中找出信息增益高于平均水平的属性，再选择增益率高的

## CART 

- CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。
- 回归树用平方误差最小化准则，分类树用基尼指数最小化准则
- 基尼指数衡量数据D的纯度，即类别标记不一致的概率

## GDBT

- 梯度下降树 
- 分类树的结果显然是没办法累加的，所以GBDT中的树都是回归树，不是分类树
- 集成学习Boosting家族的成员 
- 弱学习器限定了只能使用CART回归树模型
- 前一轮x学习器为$f_{t-1}(x)$,损失为$L(y, f_{t-1})$，本轮迭代目标是让$L(y, f_{t})=L(y, f_{t-1}+f_t)$最小
- 用损失函数的负梯度来拟合本轮损失的近似值

## GBDT与随机森林

- 都是由多棵树组成，最终的结果都是由多棵树一起决定
- 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成
- 组成随机森林的树可以并行生成；而GBDT只能是串行生成
- 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来
- 随机森林对异常值不敏感，GBDT对异常值非常敏感
- 随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成
- 随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能


# 贝叶斯

# 什么是贝叶斯估计

1. 最大似然估计, 参数是固定的，假设数据是相互独立的，对点估计

$$
\begin{aligned}
&\arg\max p(\theta|D)\\
p(\theta|D)&=\frac{p(D|\theta)p(\theta)}{p(D)}\\
&\arg\max p(D|\theta)=\prod_ip(x_i|\theta)\\
\end{aligned}
$$


2. 贝叶斯估计 对分布估计
 - 贝叶斯估计的本质是通过贝叶斯决策得到参数θ的最优估计，使得总期望风险最小
 

# 朴素贝叶斯分类器原理以及公式

  - 最小化总体风险，只需在每个样本上选择是条件风险R(c|x)最小的类别标记，既使后验概率最大
  - 转化为估计先验概率$P(c)$ (各类样本所占比例)和类条件概率(似然函数)$P(x|c)$

$$
\begin{aligned}
P(c)&=Nc/N\\
P(c|x)&=\frac{P(c)P(x|c)}{P(x)}\\
&=\frac{P(c)}{P(x)}\prod_iP(x_i|c)\\

\end{aligned}
$$

# 出现估计概率值为 0 怎么处理（拉普拉斯平滑

N个类别，$N_i$第$i$个属性的可能的取值数

$$
\begin{aligned}
P(c)&=\frac{D_c+1}{D+N}\\
P(x_i|c)&=\frac{D_{ci}+1}{Dc+N_i}\\
\end{aligned}
$$
