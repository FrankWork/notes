
# LR

## LR为什么用sigmoid函数。这个函数有什么优点和缺点？为什么不用其他函数？

- 假设第i个特征对第k类的贡献是$w_{ki}$, 则点$(x_1,..,x_n)$属于k个概率正比于$exp(w_{k1}x_1,.., w_{kn}x_n)$
- 在二分类问题上，对这个概率正则化，就是sigmoid函数，参数w_i表示第i个特征对1类的贡献比对0类的贡献多多少

$$ P(y==1)=\frac{1}{1+e^{-\sum w_ix_i}}$$

## LR和SVM有什么区别

- 不考虑核函数，LR和SVM都是线性分类算法
- 都是判别模型
- 逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值
- 支持向量机​基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面
- 支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）
- 在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的, 假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的

## Logistics与随机森林比较

 logistic回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数g(z)将最为假设函数来预测

# 随机森林

随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。

## 算法描述+bias和variance的分解公式


# HMM和CRF的本质区别

# 频率学派和贝叶斯派的本质区别

- * 频率学派认为某一事件的概率是常量，
  * 贝叶斯学派认为事件概率为变量
- * 频率学派将观测事件域中，某类事件发生的次数占总观测事件的比值近似为该事件发生的概率。
  * 贝叶斯学派则是在某些情况，如后验概率，条件概率的计算中加入超参
- * 频率学派以频率近似概率为核心思想，适合简单事件的概率分析，对于复杂的实际问题就不适用
  * 贝叶斯学派将概率本身视为变量，将该变量的超参独立出来, 可以解决频率学派无能为力的推理问题

# k近邻（knn）
找出训练集中与其最靠近的ｋ个训练样本，基于这ｋ个邻居来进行预测

回归：选取K个最近的点，然后平均（也可以加权平均）这些点y值

# k-means

K-means算法是将样本聚类成k个簇

## 与EM的关系

- 我们目的是将样本分成k个类，其实说白了就是求每个样例x的隐含类别y，然后利用隐含类别将x归类
- EM的思想，E步就是估计隐含类别y的期望值，M步调整其他参数使得在给定类别y的情况下，极大似然估计P(x,y)能够达到极大值
- 从K-means里我们可以看出它其实就是EM的体现，E步是确定隐含类别变量-最佳类别，M步更新其他参数使距离最小



# 贝叶斯

# 什么是贝叶斯估计

1. 最大似然估计, 参数是固定的，假设数据是相互独立的，对点估计

$$
\begin{aligned}
&\arg\max p(\theta|D)\\
p(\theta|D)&=\frac{p(D|\theta)p(\theta)}{p(D)}\\
&\arg\max p(D|\theta)=\prod_ip(x_i|\theta)\\
\end{aligned}
$$


2. 贝叶斯估计 对分布估计
 - 贝叶斯估计的本质是通过贝叶斯决策得到参数θ的最优估计，使得总期望风险最小
 

# 朴素贝叶斯分类器原理以及公式

  - 最小化总体风险，只需在每个样本上选择是条件风险R(c|x)最小的类别标记，既使后验概率最大
  - 转化为估计先验概率$P(c)$ (各类样本所占比例)和类条件概率(似然函数)$P(x|c)$

$$
\begin{aligned}
P(c)&=Nc/N\\
P(c|x)&=\frac{P(c)P(x|c)}{P(x)}\\
&=\frac{P(c)}{P(x)}\prod_iP(x_i|c)\\

\end{aligned}
$$

# 出现估计概率值为 0 怎么处理（拉普拉斯平滑

N个类别，$N_i$第$i$个属性的可能的取值数

$$
\begin{aligned}
P(c)&=\frac{D_c+1}{D+N}\\
P(x_i|c)&=\frac{D_{ci}+1}{Dc+N_i}\\
\end{aligned}
$$


# 集成学习

1. bagging  生成多个训练集，每个训练集训练一个学习器
2. boosting 降低偏差，根据基学习器的表现对训练数据的分布进行调整，使错误样本获得更多的关注
3. stacking 用基学习器的输出训练次级学习器


# 特征选择

1. 过滤式 
  - 相关系数，
  - 卡方检验，
  - 互信息法，
  - 信息增益
2. 包裹式 Las Vegas Wrapper，用学习器的误差作为特征子集的评价标准
3. 嵌入式 L1

# PCA

