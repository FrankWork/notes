# rnn

## basic rnn


$$ h_{t}=tanh(Wh^{t-1}+Ux^{t}+b) $$


## lstm

遗忘门，输入门, 候选值, 遗忘与更新操作，输出门，输出操作

$$ f_{t} = \sigma(W_{f} \cdot[h_{t-1},x_{t}+b_{f}]) $$
$$ i_{t} = \sigma(W_{i} \cdot[h_{t-1},x_{t}+b_{i}]) $$
$$ \tilde{C_{t}} = tanh(W_{C} \cdot[h_{t-1},x_{t}+b_{C}]) $$
$$ C_{t} = f_{t}*C_{t-1} + i_{t}*\tilde{C_{t}}$$
$$ o_{t} = \sigma(W_{o} \cdot[h_{t-1},x_{t}+b_{o}]) $$
$$ h_{t}=o_{t}*tanh(C_{t}) $$



## 梯度裁剪(梯度爆炸)

1. 当梯度的绝对值大于某个上限的时候，就剪裁为上限
2. 梯度的L2范数大于上限后，让梯度除以范数


## lstm为什么比rnn好（梯度消失）

- LSTM只能避免RNN的梯度消失
- 传统的RNN以覆盖的方式计算状态$h_{t}=f(h_{t-1},x_{t})$，导致梯度为参数的连乘积的形式，小于1的项连乘就会趋近于0.
- LSTMS使用累加的形式计算状态$h_{t}=\sum_{i=1}^{t}\Delta h_{i}$，这种累加形式的导数也是累加形式,避免了梯度消失
